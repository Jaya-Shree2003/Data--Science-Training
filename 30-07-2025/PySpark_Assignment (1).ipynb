{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Environment Setup\n",
        "1. Install Spark + Java in Google Colab.\n",
        "2. Initialize Spark with app name \"ProductSalesAnalysis\""
      ],
      "metadata": {
        "id": "LYQry4v1Zs7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Spark from the correct URL\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4sdX4v_RxNG",
        "outputId": "5ca7af30-9e26-49da-f461-a00da3e2b959"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-30 12:20:44--  https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M  19.9MB/s    in 60s     \n",
            "\n",
            "2025-07-30 12:21:45 (6.34 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Spark\n",
        "!tar -xzf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "If6-OzwuR3bY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java & Findspark\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZwZLe0PR6Y8",
        "outputId": "d9582978-663c-45be-ef48-fcb2e622e7c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 5,367 kB of archives.\n",
            "After this operation, 15.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Fetched 5,367 kB in 0s (16.3 MB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Environment Variables and Initialize Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ProductSalesAnalysis\").getOrCreate()\n",
        "spark.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lc5Zt45tR98x",
        "outputId": "ccf3bd60-bec0-419c-b786-363306a8ad40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Part 2: Load Sales Data from CSV"
      ],
      "metadata": {
        "id": "YNUP7twkZgk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the sales.csv file\n",
        "csv_data = \"\"\"OrderID,Product,Category,Quantity,UnitPrice,Region\n",
        "1001,Mobile,Electronics,2,15000,North\n",
        "1002,Laptop,Electronics,1,55000,South\n",
        "1003,T-Shirt,Apparel,3,500,East\n",
        "1004,Jeans,Apparel,2,1200,North\n",
        "1005,TV,Electronics,1,40000,West\n",
        "1006,Shoes,Footwear,4,2000,South\n",
        "1007,Watch,Accessories,2,3000,East\n",
        "1008,Headphones,Electronics,3,2500,North\"\"\"\n",
        "\n",
        "with open(\"sales.csv\", \"w\") as file:\n",
        "    file.write(csv_data)"
      ],
      "metadata": {
        "id": "X0HtIrdJSCGH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV into PySpark DataFrame\n",
        "df = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "wA0P7ZECSE7U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Schema and Show Top 5 Rows\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYbXVb83SFlO",
        "outputId": "c37caccb-802e-4fba-a983-1caa359ee258"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- UnitPrice: integer (nullable = true)\n",
            " |-- Region: string (nullable = true)\n",
            "\n",
            "+-------+-------+-----------+--------+---------+------+\n",
            "|OrderID|Product|   Category|Quantity|UnitPrice|Region|\n",
            "+-------+-------+-----------+--------+---------+------+\n",
            "|   1001| Mobile|Electronics|       2|    15000| North|\n",
            "|   1002| Laptop|Electronics|       1|    55000| South|\n",
            "|   1003|T-Shirt|    Apparel|       3|      500|  East|\n",
            "|   1004|  Jeans|    Apparel|       2|     1200| North|\n",
            "|   1005|     TV|Electronics|       1|    40000|  West|\n",
            "+-------+-------+-----------+--------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part -3 Business Questions\n",
        "1. Add a new column TotalPrice = Quantity × UnitPrice\n",
        "2. Total revenue generated across all regions.\n",
        "3. Category-wise revenue sorted in descending order.\n",
        "4. Region with the highest number of orders\n",
        "5. Average Unit Price per Category\n",
        "6. All orders where TotalPrice is more than\n",
        "30,000"
      ],
      "metadata": {
        "id": "hXtrrOF_YE3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Add a new column TotalPrice = Quantity × UnitPrice\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"TotalPrice\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq00NzopSK40",
        "outputId": "4cbea049-c298-4609-9945-bf392fd7e7a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+--------+---------+------+----------+\n",
            "|OrderID|   Product|   Category|Quantity|UnitPrice|Region|TotalPrice|\n",
            "+-------+----------+-----------+--------+---------+------+----------+\n",
            "|   1001|    Mobile|Electronics|       2|    15000| North|     30000|\n",
            "|   1002|    Laptop|Electronics|       1|    55000| South|     55000|\n",
            "|   1003|   T-Shirt|    Apparel|       3|      500|  East|      1500|\n",
            "|   1004|     Jeans|    Apparel|       2|     1200| North|      2400|\n",
            "|   1005|        TV|Electronics|       1|    40000|  West|     40000|\n",
            "|   1006|     Shoes|   Footwear|       4|     2000| South|      8000|\n",
            "|   1007|     Watch|Accessories|       2|     3000|  East|      6000|\n",
            "|   1008|Headphones|Electronics|       3|     2500| North|      7500|\n",
            "+-------+----------+-----------+--------+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Total revenue generated across all regions.\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "\n",
        "df.agg(sum(\"TotalPrice\").alias(\"TotalRevenue\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqvF84rgSN6u",
        "outputId": "13237e89-232a-4dcc-f11c-8733b240517c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|TotalRevenue|\n",
            "+------------+\n",
            "|      150400|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Category-wise revenue sorted in descending order.\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "category_revenue = df.groupBy(\"Category\").agg(sum(\"TotalPrice\").alias(\"TotalRevenue\"))\n",
        "category_revenue = category_revenue.orderBy(desc(\"TotalRevenue\"))\n",
        "category_revenue.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGxTFs2hSQih",
        "outputId": "e1296326-a8fb-4af6-c948-9ae8a6679497"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|   Category|TotalRevenue|\n",
            "+-----------+------------+\n",
            "|Electronics|      132500|\n",
            "|   Footwear|        8000|\n",
            "|Accessories|        6000|\n",
            "|    Apparel|        3900|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Region with the highest number of orders\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "region_order_count = df.groupBy(\"Region\").count()\n",
        "region_order_count = region_order_count.orderBy(desc(\"count\"))\n",
        "region_order_count.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ9gjUEESS7B",
        "outputId": "234b0a52-f0f7-4932-e7b5-dead62bc9afc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|Region|count|\n",
            "+------+-----+\n",
            "| North|    3|\n",
            "+------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Average Unit Price per Category\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "average_price_per_category = df.groupBy(\"Category\").agg(avg(\"UnitPrice\").alias(\"AverageUnitPrice\"))\n",
        "average_price_per_category.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSi7xJIzSV53",
        "outputId": "ceaf6972-9903-4e25-8452-f91ceda281ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+\n",
            "|   Category|AverageUnitPrice|\n",
            "+-----------+----------------+\n",
            "|    Apparel|           850.0|\n",
            "|Electronics|         28125.0|\n",
            "|   Footwear|          2000.0|\n",
            "|Accessories|          3000.0|\n",
            "+-----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. All orders where TotalPrice is more than 30,000\n",
        "filtered_orders = df.filter(col(\"TotalPrice\") > 30000)\n",
        "filtered_orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn-lvSe-SZAv",
        "outputId": "ebb5d98d-b55f-41f8-f56e-299af2dccd33"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----------+--------+---------+------+----------+\n",
            "|OrderID|Product|   Category|Quantity|UnitPrice|Region|TotalPrice|\n",
            "+-------+-------+-----------+--------+---------+------+----------+\n",
            "|   1002| Laptop|Electronics|       1|    55000| South|     55000|\n",
            "|   1005|     TV|Electronics|       1|    40000|  West|     40000|\n",
            "+-------+-------+-----------+--------+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4Data Transformations\n",
        "1. Create a new column HighValueOrder which is \"Yes\" if TotalPrice > 20,000,\n",
        "else \"No\" .\n",
        "2. Filter and display all high-value orders in the North region.\n",
        "3. Count how many high-value orders exist per region."
      ],
      "metadata": {
        "id": "C-ODmNDdY2jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import col, when, count\n",
        "\n",
        "\n",
        "# 1. HighValueOrder column\n",
        "df = df.withColumn(\"HighValueOrder\", when(col(\"TotalPrice\") > 20000, \"Yes\").otherwise(\"No\"))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amzkb8wZSY4I",
        "outputId": "bddaf608-c78c-4a67-ecaa-8cf521b86ef1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+--------+---------+------+----------+--------------+\n",
            "|OrderID|   Product|   Category|Quantity|UnitPrice|Region|TotalPrice|HighValueOrder|\n",
            "+-------+----------+-----------+--------+---------+------+----------+--------------+\n",
            "|   1001|    Mobile|Electronics|       2|    15000| North|     30000|           Yes|\n",
            "|   1002|    Laptop|Electronics|       1|    55000| South|     55000|           Yes|\n",
            "|   1003|   T-Shirt|    Apparel|       3|      500|  East|      1500|            No|\n",
            "|   1004|     Jeans|    Apparel|       2|     1200| North|      2400|            No|\n",
            "|   1005|        TV|Electronics|       1|    40000|  West|     40000|           Yes|\n",
            "|   1006|     Shoes|   Footwear|       4|     2000| South|      8000|            No|\n",
            "|   1007|     Watch|Accessories|       2|     3000|  East|      6000|            No|\n",
            "|   1008|Headphones|Electronics|       3|     2500| North|      7500|            No|\n",
            "+-------+----------+-----------+--------+---------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. High-value orders in North region\n",
        "df.filter((col(\"HighValueOrder\") == \"Yes\") & (col(\"Region\") == \"North\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo4h0we2Y2Mg",
        "outputId": "6101ff91-4dae-4ab6-9c9d-cb4c933e103b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----------+--------+---------+------+----------+--------------+\n",
            "|OrderID|Product|   Category|Quantity|UnitPrice|Region|TotalPrice|HighValueOrder|\n",
            "+-------+-------+-----------+--------+---------+------+----------+--------------+\n",
            "|   1001| Mobile|Electronics|       2|    15000| North|     30000|           Yes|\n",
            "+-------+-------+-----------+--------+---------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Count high-value orders per region\n",
        "df.filter(col(\"HighValueOrder\") == \"Yes\") \\\n",
        "  .groupBy(\"Region\") \\\n",
        "  .agg(count(\"OrderID\").alias(\"HighValueOrderCount\")) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbo2z0Q4ZCi2",
        "outputId": "3aab816e-1193-4ea1-801e-2bedd1cac9d1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+\n",
            "|Region|HighValueOrderCount|\n",
            "+------+-------------------+\n",
            "| South|                  1|\n",
            "|  West|                  1|\n",
            "| North|                  1|\n",
            "+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Save Results\n",
        "Save the transformed DataFrame as a CSV file named high_value_orders.csv with\n",
        "headers."
      ],
      "metadata": {
        "id": "P3qleu0rZSV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save high-value orders to CSV\n",
        "df.filter(col(\"HighValueOrder\") == \"Yes\") \\\n",
        "  .write.csv(\"high_value_orders.csv\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# View saved folder in Colab\n",
        "!ls high_value_orders.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HyOowQsZR19",
        "outputId": "99b523d7-0e55-4015-da5d-9d4a1e95c34a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-71969407-b470-4ca3-8bb6-49c518991083-c000.csv  _SUCCESS\n"
          ]
        }
      ]
    }
  ]
}